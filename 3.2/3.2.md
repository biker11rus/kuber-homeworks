# Домашнее задание к занятию "Установка кластера K8s"

### Цель задания

Установить кластер K8s.

### Чеклист готовности к домашнему заданию

1. Развернутые ВМ с ОС Ubuntu 20.04-lts


### Инструменты и дополнительные материалы, которые пригодятся для выполнения задания

1. [Инструкция по установке kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/)
2. [Документация kubespray](https://kubespray.io/)

-----

### Задание 1. Установить кластер k8s с 1 master node

1. Подготовка работы кластера из 5 нод: 1 мастер и 4 рабочие ноды.
2. В качестве CRI — containerd.
3. Запуск etcd производить на мастере.
4. Способ установки выбрать самостоятельно.

## *Ответ*

1. Склонирован репозиторий kubespray, создана среда с помощью Virtualenv, установлены зависимости и создана копия конфигурации 
```bash
git clone https://github.com/kubernetes-sigs/kubespray
virtualenv  --python=$(which python3) kubespray-venv
source kubespray-venv/bin/activate
pip3 install -r requirements.txt
cp -rfp inventory/sample inventory/mycluster
```
2.  Созданы 5 ВМ в Яндекс Облаке с помощью terraform main.tf. После создания ВМ автоматически создается inventory.ini с помощью шаблона inventory.tpl.   
Метадата для ВМ  создаются с помощью шаблона cloud_init_config.tpl. Переменные в variables.tf  

main.tf

```
terraform {
  required_providers {
    yandex = {
      source  = "yandex-cloud/yandex"
      version = "~> 0.61.0"
    }
  }
}
provider "yandex" {
  service_account_key_file = "key.json"
  cloud_id                 = var.yandex_cloud_id
  folder_id                = var.yandex_folder_id
  zone                     = var.yandex_zone
}
module "vpc" {
  source       = "hamnsk/vpc/yandex"
  version      = "0.5.0"
  description  = "managed by terraform"
  yc_folder_id = var.yandex_folder_id
  name         = "yc_vpc2"
  subnets      = local.vpc_subnets.yc_sub
}
locals {
  vpc_subnets = {
    yc_sub = [
      {
        "v4_cidr_blocks" : [
          "10.128.0.0/24"
        ],
        "zone" : var.yandex_zone
      }
    ]
  }
}

locals {
  instance_set = {
    node1    = "kube_control_plane"
    node2    = "kube_node"
    node3    = "kube_node"
    node4    = "kube_node"
    node5    = "kube_node"
  }
}


data "template_file" "cloud_init" {
  template = file("cloud_init_config.tpl")
  vars = {
    user    = var.user
    ssh_key = file(var.public_key_path)
  }
}
resource "yandex_compute_instance" "vms" {
  for_each = local.instance_set
  name     = each.key
  resources {
    cores  = 2
    memory = 4
  }
  boot_disk {
    initialize_params {
      image_id = var.iso_id
      size     = 20
    }
  }
  network_interface {
    subnet_id = module.vpc.subnet_ids[0]
    nat       = true
  }
  metadata = {
    user-data = data.template_file.cloud_init.rendered
  }
  labels = {
    ansible-group = each.value
  }
}
resource "local_file" "AnsibleInventory" {
  content = templatefile("inventory.tpl",
    {
      vm-names      = [for k, p in yandex_compute_instance.vms : p.name],
      private-ip    = [for k, p in yandex_compute_instance.vms : p.network_interface.0.ip_address],
      public-ip     = [for k, p in yandex_compute_instance.vms : p.network_interface.0.nat_ip_address],
      ansible-group = [for k, p in yandex_compute_instance.vms : p.labels.ansible-group],
      ssh_user      = var.user
    }
  )
  filename = "../kubespray/inventory/mycluster/inventory.ini"
}
```

inventory.tpl  
```
[all]
%{ for index, vms in vm-names ~}
${vms} ansible_host=${public-ip[index]} ip=${private-ip[index]}
%{ endfor ~}

[kube_control_plane]
%{ for indexgp, group in ansible-group ~}
%{ if group == "kube_control_plane" ~}
${vm-names[indexgp]}
%{ endif ~}
%{ endfor ~}

[etcd]
%{ for indexgp, group in ansible-group ~}
%{ if group == "kube_control_plane" ~}
${vm-names[indexgp]}
%{ endif ~}
%{ endfor ~}

[kube_node]
%{ for indexgp, group in ansible-group ~}
%{ if group == "kube_node" ~}
${vm-names[indexgp]}
%{ endif ~}
%{ endfor ~}


[calico_rr]

[k8s_cluster:children]
kube_control_plane
kube_node
calico_rr
```

cloud_init_config.tpl  
```
#cloud-config
users:
  - name: ${user}
    groups: sudo
    shell: /bin/bash
    sudo: ['ALL=(ALL) NOPASSWD:ALL']
    ssh-authorized-keys:
      - ${ssh_key}
```

Итоговый inventory.ini

```
[all]
node1 ansible_host=84.201.129.241 ip=10.128.0.19
node2 ansible_host=62.84.112.171 ip=10.128.0.27
node3 ansible_host=62.84.116.116 ip=10.128.0.3
node4 ansible_host=51.250.7.30 ip=10.128.0.24
node5 ansible_host=51.250.80.146 ip=10.128.0.17

[kube_control_plane]
node1

[etcd]
node1

[kube_node]
node2
node3
node4
node5


[calico_rr]

[k8s_cluster:children]
kube_control_plane
kube_node
calico_rr

```

3. Для подключения из вне добавил реальник control-node в файл  k8s-cluster.yml

```bash
< # supplementary_addresses_in_ssl_keys: [10.0.0.1, 10.0.0.2, 10.0.0.3]
---
> supplementary_addresses_in_ssl_keys: [84.201.129.241]
```
4. Установка кластера
   
```bash
ansible-playbook -i inventory/mycluster/inventory.ini cluster.yml -b
```
6. Проверка с control node

```bash
rkhozyainov@node1:~$ sudo kubectl get nodes
NAME    STATUS   ROLES           AGE   VERSION
node1   Ready    control-plane   18m   v1.26.3
node2   Ready    <none>          17m   v1.26.3
node3   Ready    <none>          17m   v1.26.3
node4   Ready    <none>          17m   v1.26.3
node5   Ready    <none>          17m   v1.26.3
```

7. Для подключения с хоста создан файл config и заполнен данными из /etc/kubernetes/admin.conf с control_node: certificate-authority-data, client-certificate-data, client-key-data. Указан ip control_node

config

```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: 
    server: https://84.201.129.241:6443
  name: test_cluster
contexts:
- context:
    cluster: test_cluster
    user: kubernetes-admin
  name: kubernetes-admin@test_cluster
current-context: kubernetes-admin@test_cluster
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: 
    client-key-data: 
```
Проверка

```bash
rkhozyainov@rkhozyainov-T530-ubuntu:~/devops/Other/test_k8s_cluster$ kubectl get nodes -o wide --kubeconfig=./config 
NAME    STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
node1   Ready    control-plane   64m   v1.26.3   10.128.0.19   <none>        Ubuntu 20.04.4 LTS   5.4.0-109-generic   containerd://1.7.0
node2   Ready    <none>          63m   v1.26.3   10.128.0.27   <none>        Ubuntu 20.04.4 LTS   5.4.0-109-generic   containerd://1.7.0
node3   Ready    <none>          63m   v1.26.3   10.128.0.3    <none>        Ubuntu 20.04.4 LTS   5.4.0-109-generic   containerd://1.7.0
node4   Ready    <none>          63m   v1.26.3   10.128.0.24   <none>        Ubuntu 20.04.4 LTS   5.4.0-109-generic   containerd://1.7.0
node5   Ready    <none>          63m   v1.26.3   10.128.0.17   <none>        Ubuntu 20.04.4 LTS   5.4.0-109-generic   containerd://1.7.0
```


